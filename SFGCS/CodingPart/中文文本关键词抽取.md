## 中文文本关键词抽取

> 参考网站：https://github.com/AimeeLee77/keyword_extraction
>
> ​					https://github.com/AimeeLee77/wiki_zh_word2vec
>
> 主要工具包：Jieba、Gensim、Pandas、Numpy、Scikit-learn、Matplotlib、codecs
>
> 涉及方法：TF-IDF、TextRank、KMeans
>
> 数据来源：E:\Pycharm_Project\SFGCS\keyword_extraction\data;
>
> ​					Wiki中文语料
>
> 项目位置：服务器100，/home/yht/fengyr_file/SFGCS/keyword_extraction/

### TF-IDF

##### 方法原理

**词频**（TF，Term Frequency）指某一给定词语在当前文件中出现的频率。由于同一个词语在长文件中可能比短文件中有更高的词频，因此根据文件的长度，需要对给定词语进行归一化，即用给定词语的次数除以当前文件的总词数

**逆向文件频率**（IDF，Inverse Document Frequency）是一个词语普遍重要性的度量。即如果一个词语只在很少的文件中出现，表示更能代表文件的主旨，它的权重也就越大；如果一个词在大量文件中都出现，表示不清楚代表什么内容，它的权重就应该小。

TF-IDF的**主要思想**是，如果某个词语在一篇文章中出现的频率高，并且在其他文章中较少出现，则认为该词语能较好的代表当前文章的含义。即一个词语的重要性与它在文档中出现的次数成正比，与它在语料库中文档出现的频率成反比。

计算公式如下：
$$
TF=\frac{词在文档中出现的次数}{文档的总词数}\\
IDF=log(\frac{语料库的文档总数}{包含词的文档数+1})\\
TF-IDF=TF*IDF
$$

##### 优劣

##### TF-IDF文本关键词抽取方法流程

设$D_n$为测试语料的大小

（1）对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。利用jieba第三方工具包，保留'n','nz','v','vd','vn','l','a','d'这几个词性的词语，最终得到n个候选关键词，即$D=[t_1,t_2,...,t_n]$。

（2）计算词语$t_i$在文本D中的词频；

（3）计算词语$t_i$在整个语料的IDF；

（4）计算得到词语$t_i$的TF-IDF，并重复（2）-（4）得到所有候选关键词的TF-IDF数值；

（5）对候选关键词计算结果进行倒序排列，得到排名TopN个词汇作为文本关键词。

##### sklearn实现代码

```python
# coding=utf-8
# 采用TF-IDF方法提取文本关键词
# http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting
import sys,codecs
import pandas as pd
import numpy as np
import jieba.posseg
import jieba.analyse
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
"""
       TF-IDF权重：
           1、CountVectorizer 构建词频矩阵
           2、TfidfTransformer 构建tfidf权值计算
           3、文本的关键字
           4、对应的tfidf矩阵
"""
# 数据预处理操作：分词，去停用词，词性筛选
def dataPrepos(text, stopkey):
    l = []
    pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd']  # 定义选取的词性
    seg = jieba.posseg.cut(text)  # 分词
    for i in seg:
        if i.word not in stopkey and i.flag in pos:  # 去停用词 + 词性筛选
            l.append(i.word)
    return l

# tf-idf获取文本top10关键词
def getKeywords_tfidf(data,stopkey,topK):
    idList, titleList, abstractList = data['id'], data['title'], data['abstract']
    corpus = [] # 将所有文档输出到一个list中，一行就是一个文档
    for index in range(len(idList)):
        text = '%s。%s' % (titleList[index], abstractList[index]) # 拼接标题和摘要
        text = dataPrepos(text,stopkey) # 文本预处理
        text = " ".join(text) # 连接成字符串，空格分隔
        corpus.append(text)

    # 1、构建词频矩阵，将文本中的词语转换成词频矩阵
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频
    # 2、统计每个词的tf-idf权值
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(X)
    # 3、获取词袋模型中的关键词
    word = vectorizer.get_feature_names()
    # 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重
    weight = tfidf.toarray()
    # 5、打印词语权重
    ids, titles, keys = [], [], []
    for i in range(len(weight)):
        print(u"-------这里输出第", i+1 , u"篇文本的词语tf-idf------")
        ids.append(idList[i])
        titles.append(titleList[i])
        df_word,df_weight = [],[] # 当前文章的所有词汇列表、词汇对应权重列表
        for j in range(len(word)):
            print(word[j],weight[i][j])
            df_word.append(word[j])
            df_weight.append(weight[i][j])
        df_word = pd.DataFrame(df_word,columns=['word'])
        df_weight = pd.DataFrame(df_weight,columns=['weight'])
        word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表
        word_weight = word_weight.sort_values(by="weight",ascending = False) # 按照权重值降序排列
        keyword = np.array(word_weight['word']) # 选择词汇列并转成数组格式
        word_split = [keyword[x] for x in range(0,topK)] # 抽取前topK个词汇作为关键词
        word_split = " ".join(word_split)
        keys.append(word_split.encode("utf-8"))
        #感觉上一行 可加可不加 实验发现不用加encode("utf-8")就能显示中文

    result = pd.DataFrame({"id": ids, "title": titles, "key": keys},columns=['id','title','key'])
    return result


def main():
    # 读取数据集
    dataFile = 'data/sample_data.csv'
    data = pd.read_csv(dataFile)
    # 停用词表
    stopkey = [w.strip() for w in codecs.open('data/stopWord.txt', 'r').readlines()]
    # tf-idf关键词抽取
    result = getKeywords_tfidf(data,stopkey,10)
    result.to_csv("result/keys_TFIDF.csv",index=False)

if __name__ == '__main__':
    main()
```

##### 使用Gensim工具包实现TF-IDF算法



### PageRank&TextRank

TextRank算法是基于PageRank算法的，因此，在介绍TextRank前不得不了解一下PageRank算法。

##### PageRank算法

**原理**：根据页间相互的超链接来计算网页重要性的技术。该算法借鉴了学术届评判学术论文重要性的方法，即查看论文的被引用次数。基于以上想法，PageRank算法的核心思想是，认为网页重要性由两部分组成：1.如果一个网页被大量其他网页链接到说明这个网页比较重要，即被链接网页的数量；2.如果一个网页被排名很高的网页链接说明这个网页比较重要，即被链接网页的权重。

一般情况下，一个网页的PageRank值（PR）计算公式如下：
$$
PR(p_i)=\frac{1-\alpha}{N}+\alpha\sum_{p_j\in M_{p_i}}\frac{PR(p_j)}{L(p_j)}
$$
其中，$PR(p_i)$是第$i$个网页的重要性排名即PR值；$\alpha$是阻尼系数，一般设置为0.85；N是网页总数；$M_{p_i}$是所有对第$i$个网站有出链的网页集合；$L(p_j)$是第j个网页的出链数目。

初始时，假设所有网页的排名都是1/N，根据上述公式计算出每个网页的PR值，在不断迭代趋于平稳的时候，停止迭代运算，得到最终结果。一般来讲，只要10次左右的迭代基本上就收敛了。

##### TextRank算法

TextRank算法是Mihalcea和Tarau于2004年在研究自动摘要提取过程中所提出来的，在PageRank算法的思路上做了改进。该算法把文本拆分成词汇作为网络节点，组成词汇网络图模型，将词语间的相似关系看成是一种推荐或投票关系，使其可以计算每一个词语的重要性。

基于TextRank的文本关键词抽取是利用局部词汇关系，即共现窗口，对候选关键词进行排序，该方法的步骤如下：

（1）对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。利用jieba分词工具，保留'n','nz','v','vd','vn','l','a','d'这几个词性的词语，最终得到n个候选关键词，即$D=[t_1,t_2,...,t_n]$

（2）构建候选关键词图$G=(V,E)$，其中V为节点集，由候选关键词组成，并采用共现关系构造两点之间的边，两个节点之间仅当它们对应的词汇在长度为K的窗口中共现则存在边，K表示窗口大小即最多共现K个词汇。

（3）根据公式迭代计算各节点的权重，直至收敛；

（4）对节点权重进行倒序排列，得到排名前TopN个词汇作为文本关键词。

说明：Jieba库中包含jieba.analyse.textrank函数可直接实现TextRank算法。

##### 除了关键词图的其他实现手段

##### TextRank代码实现

```python
#!/usr/bin/python
# coding=utf-8
# 采用TextRank方法提取文本关键词
import sys
import pandas as pd
import jieba.analyse
"""
       TextRank权重：

            1、将待抽取关键词的文本进行分词、去停用词、筛选词性
            2、以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图
            3、计算图中节点的PageRank，注意是无向带权图
"""

# 处理标题和摘要，提取关键词
def getKeywords_textrank(data,topK):
    idList,titleList,abstractList = data['id'],data['title'],data['abstract']
    ids, titles, keys = [], [], []
    for index in range(len(idList)):
        text = '%s。%s' % (titleList[index], abstractList[index]) # 拼接标题和摘要
        jieba.analyse.set_stop_words("data/stopWord.txt") # 加载自定义停用词表
        print("\"",titleList[index],"\"" , " 10 Keywords - TextRank :")
        keywords = jieba.analyse.textrank(text, topK=topK, allowPOS=('n','nz','v','vd','vn','l','a','d'))  # TextRank关键词提取，词性筛选
        word_split = " ".join(keywords)
        print(word_split)
        keys.append(word_split.encode("utf-8"))
        ids.append(idList[index])
        titles.append(titleList[index])

    result = pd.DataFrame({"id": ids, "title": titles, "key": keys}, columns=['id', 'title', 'key'])
    return result

def main():
    dataFile = 'data/sample_data.csv'
    data = pd.read_csv(dataFile)
    result = getKeywords_textrank(data,10)
    result.to_csv("result/keys_TextRank.csv",index=False)

if __name__ == '__main__':
    main()
```

### Word2Vec & K-means词聚类

K-Means是一种常见的基于原型的聚类技术，本文选择该算法作为词聚类的方法。其算法思想是：首先随机选择K个点作为初始质心，K为用户指定的所期望的簇的个数，通过计算每个点到各个质心的距离，将每个点指派到最近的质心形成K个簇，然后根据指派到簇的点重新计算每个簇的质心，重复指派和更新质心的操作，直到簇不发生变化或达到最大的迭代次数则停止。

##### K-Means文本关键词抽取方法流程

通过K-Means算法对文章中的词进行聚类，选择聚类中心作为文章的一个主要关键词，计算其他词与聚类中心的距离即相似度，选择topN个距离聚类中心最近的词作为文本关键词，而这个词间相似度可用Word2vec生成的向量计算得到。

假设$D_n$为测试语料的大小，使用该方法进行文本关键词抽取的步骤如下所示：

（1）对Wiki中文语料进行Word2vec模型训练，[参考文章](http://www.jianshu.com/p/ec27062bd453 )，得到词向量文件"wiki.zh.text.vector"；

（2）对于给定的文本D进行分词、词性标注、去重和去除停用词等数据预处理操作。采用jieba第三方库保留'n','nz','v','vd','vn','l','a','d'这几个词性的词语，最终得到n个候选关键词，即$D=[t_1,t_2,...,t_n]$；

（3）遍历候选关键词，从词向量文件中抽取候选关键词的词向量表示，即$V=[v_1,v_2,...,v_m]$；

（4）对候选关键词进行K-Means聚类，得到各个类别的聚类中心；

（5）计算各类别下，组内词语与聚类中心的距离（欧几里得距离），按聚类大小进行升序排序；

（6）对候选关键词计算结果得到排名前topN个词汇作为文本关键词。

其中，欧式距离计算公式为：
$$
d =\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}
$$

##### 代码实现

Python第三方工具包Scikit-learn提供了K-Means聚类算法的相关函数，本文用到了sklearn.cluster.KMeans()函数执行K-Means算法，sklearn.decomposition.PCA()函数用于数据降维以便绘制图形。

###### 一、Wiki数据获取

到wiki官网下载中文语料，下载完成后会得到命名为zhwiki-latest-pages-articles.xml.bz2的文件，大小约为1.3G，里面是一个XML文件。 下载地址如下：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2

###### 二、将XML的Wiki数据转换为text格式

```python
#!/usr/bin/env python
# -*- coding: utf-8  -*-
#将xml的wiki数据转换为text格式

import logging
import os.path
import sys

from gensim.corpora import WikiCorpus

if __name__ == '__main__':
    outp = 'data/wiki.zh.txt'
    inp = 'data/zhwiki-latest-pages-articles.xml.bz2'
    i = 0

    output = open(outp, 'w')
    wiki =WikiCorpus(inp, dictionary=[])#gensim里的维基百科处理类WikiCorpus
    for text in wiki.get_texts():#通过get_texts将维基里的每篇文章转换位1行text文本，并且去掉了标点符号等内容
        output.write(" ".join(text) + "\n")
        i = i+1
        if (i % 10000 == 0):
            print("Saved "+str(i)+" articles.")

    output.close()
    print("Finished Saved "+str(i)+" articles.")
```

###### 三、Wiki数据预处理

**3.1 中文繁体替换成简体**

Wiki中文语料中包含了很多繁体字，需要转成简体字再进行处理，这里使用到了OpenCC工具进行转换。

```python
#!/usr/bin/env python
# -*- coding: utf-8  -*-
#将xml的wiki数据转换为text格式

import os.path
import sys
from gensim.corpora import WikiCorpus
from opencc import OpenCC

if __name__ == '__main__':
    data_path = 'data/wiki.zh.txt'
    data_new_path = 'data/wiki2simple.zh.txt'
    C = OpenCC('t2s')
    with open(data_path, 'r',encoding="utf8") as f:
        with open(data_new_path, 'w',encoding="utf8") as n:
            text = f.readline()
            while text:
                data_new = C.convert(text)
                n.writelines(data_new)
                text = f.readline()
    print('t2s is over!')

    #------------------------------------------------------------------------
    # data_path = 'data/wiki2simple.zh.txt'
    # with open(data_path, 'r',encoding='utf-8') as f:
    #     for i in range(10):
    #         text = f.readline()
    #     print(text)
```

**3.2 jieba分词**

由于此语料已经去除了标点符号，因此在分词程序中无需进行清洗操作，可直接分词。若是自己采集的数据还需进行标点符号去除和去除停用词的操作。

```python
#!/usr/bin/env python
# -*- coding: utf-8  -*-
#逐行读取文件数据进行jieba分词

import jieba
import jieba.analyse
import jieba.posseg as pseg #引入词性标注接口 
import codecs,sys


if __name__ == '__main__':
    f = codecs.open('wiki.zh.simp.txt', 'r', encoding='utf8')
    target = codecs.open('wiki.zh.simp.seg.txt', 'w', encoding='utf8')
    print 'open files.'

    lineNum = 1
    line = f.readline()
    while line:
        print '---processing ',lineNum,' article---'
        seg_list = jieba.cut(line,cut_all=False)
        line_seg = ' '.join(seg_list)
        target.writelines(line_seg)
        lineNum = lineNum + 1
        line = f.readline()

    print 'well done.'
    f.close()
    target.close()
```

###### 四、模型训练

利用gensim.models模块中的word2vec训练。

```python
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

if __name__ == '__main__':
    inp = 'data/wiki2final.zh.txt'
    outp1 = 'result/wiki.zh.text.model'
    outp2 = 'result/wiki.zh.text.vector'

    # 训练skip-gram模型
    model = Word2Vec(LineSentence(inp), vector_size=400, window=5, min_count=5)

    # 保存模型
    model.save(outp1)
    model.wv.save_word2vec_format(outp2, binary=False)
    print("is over")
```

加载预训练模型

```python
import gensim
from gensim.models import Word2Vec

if __name__ == '__main__':
    model = gensim.models.Word2Vec.load('result/wiki.zh.text.model')

    # word = model.wv.most_similar(u"足球")
    # for t in word:
    #     print(t[0], t[1])
    word = model.wv.most_similar(positive=[u'皇上',u'国王'],negative=[u'皇后'])
    for t in word:
        print(t[0],t[1])
    print(model.wv.doesnt_match(u'太后 妃子 贵人 贵妃 才人'.split()))
    print(model.wv.similarity(u'书籍',u'书本'))
    print(model.wv.similarity(u'逛街',u'书本'))
```

###### 五、获取文本词向量表示

步骤一至四生成词汇的向量表示，步骤五将data中分词后的词汇映射为对应的向量

```python
import sys, codecs
import numpy as np
import pandas as pd
import jieba
import jieba.posseg
import gensim

def getWordVecs(wordlist, vectors):
    name = []
    vecs = []
    for word in wordlist:
        word = word.replace('\n', '')
        try:
            if word in vectors:
                name.append(word.encode('utf-8'))
                vecs.append(vectors[word])
        except KeyError:
            continue
    a = pd.DataFrame(name, columns=['word'])
    b = pd.DataFrame(np.array(vecs, dtype='float'))
    print(a)
    print(b)
    return pd.concat([a,b], axis=1)

def dataprocess(text, stopkey):
    l = []
    pos = ['n', 'nz','v','vd','vn','l','a','d']
    seg = jieba.posseg.cut(text)
    for i in seg:
        if i.word not in stopkey and i.flag in pos:
            l.append(i.word)
    return l


def buildAllWordsVecs(data, stopkey, vectors):
    idList, titleList, abstractList = data['id'], data['title'], data['abstract']
    for i in range(len(idList)):
        title = titleList[i]
        abstract = abstractList[i]
        pro_title = dataprocess(title, stopkey)
        pro_abstract = dataprocess(abstract, stopkey)
        words = np.append(pro_title, pro_abstract)
        words = list(set(words))
        wordvecs = getWordVecs(words, vectors)
        data_vecs = pd.DataFrame(wordvecs)
        data_vecs.to_csv('result/vecs/wordvecs_' + str(id) + '.csv', index=False)
        print("document", id, "well done")

def main():
    data_path = 'data/sample_data.csv'
    data = pd.read_csv(data_path)

    stopkey = [w.strip() for w in codecs.open('data/stopWord.txt', 'r').readlines()]
    inp = 'result/wiki.zh.text.vector'
    vectors = gensim.models.KeyedVectors.load_word2vec_format(inp, binary=False)
    buildAllWordsVecs(data, stopkey, vectors)

if __name__ == '__main__':
    main()
```

###### 六、根据候选关键词的词向量进行聚类分析

